% Template file for an a0 landscape poster.
% Written by Graeme, 2001-03 based on Norman's original microlensing
% poster.
%
% See discussion and documentation at
% <http://www.astro.gla.ac.uk/users/norman/docs/posters/> 
%
% $Id: poster-template-landscape.tex,v 1.2 2002/12/03 11:25:46 norman Exp $


% Default mode is landscape, which is what we want, however dvips and
% a0poster do not quite do the right thing, so we end up with text in
% landscape style (wide and short) down a portrait page (narrow and
% long). Printing this onto the a0 printer chops the right hand edge.
% However, 'psnup' can save the day, reorienting the text so that the
% poster prints lengthways down an a0 portrait bounding box.
%
% 'psnup -w85cm -h119cm -f poster_from_dvips.ps poster_in_landscape.ps'

\documentclass[a0]{a0poster}
% You might find the 'draft' option to a0 poster useful if you have
% lots of graphics, because they can take some time to process and
% display. (\documentclass[a0,draft]{a0poster})
\input defs
\pagestyle{empty}
\setcounter{secnumdepth}{0}
\renewcommand{\familydefault}{\sfdefault}
\newcommand{\QED}{~~\rule[-1pt]{8pt}{8pt}}\def\qed{\QED}

\renewcommand{\reals}{{\mbox{\bf R}}}

% The textpos package is necessary to position textblocks at arbitary 
% places on the page.
\usepackage[absolute]{textpos}

\usepackage{fleqn,psfrag,wrapfig,tikz}
\usepackage{amsmath, xparse}

\usepackage{animate}
\usepackage{wrapfig} 
\usepackage{caption}



\usepackage[papersize={38in,28in}]{geometry}

% Graphics to include graphics. Times is nice on posters, but you
% might want to switch it off and go for CMR fonts.
\usepackage{graphics}


% we are running pdflatex, so convert .eps files to .pdf
%\usepackage[pdftex]{graphicx}
%\usepackage{epstopdf}

% These colours are tried and tested for titles and headers. Don't
% over use color!
\usepackage{color}
\definecolor{Red}{rgb}{0.9,0.0,0.1}

\definecolor{bluegray}{rgb}{0.15,0.20,0.40}
\definecolor{bluegraylight}{rgb}{0.35,0.40,0.60}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{lightgray}{rgb}{0.7,0.7,0.7}
\definecolor{darkblue}{rgb}{0.2,0.2,1.0}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.3}

\renewcommand{\labelitemi}{\textcolor{bluegray}\textbullet}
\renewcommand{\labelitemii}{\textcolor{bluegray}{--}}

\setlength{\labelsep}{0.5em}


% see documentation for a0poster class for the size options here
\let\Textsize\normalsize
%\def\Head#1{\noindent\hbox to \hsize{\hfil{\LARGE\color{bluegray} #1}}\bigskip}
\def\Head#1{\noindent{\LARGE\color{bluegray} #1}\bigskip}
\def\LHead#1{\noindent{\LARGE\color{bluegray} #1}\bigskip}
\def\Subhead#1{\noindent{\large\color{bluegray} #1}\bigskip}
\def\Title#1{\noindent{\VeryHuge\color{Red} #1}}


% Set up the grid
%
% Note that [40mm,40mm] is the margin round the edge of the page --
% it is _not_ the grid size. That is always defined as 
% PAGE_WIDTH/HGRID and PAGE_HEIGHT/VGRID. In this case we use
% 23 x 12. This gives us three columns of width 7 boxes, with a gap of
% width 1 in between them. 12 vertical boxes is a good number to work
% with.
%
% Note however that texblocks can be positioned fractionally as well,
% so really any convenient grid size can be used.
%
\TPGrid[40mm,40mm]{23}{12}      % 3 cols of width 7, plus 2 gaps width 1

\parindent=0pt
\parskip=0.2\baselineskip

\begin{document}

% Understanding textblocks is the key to being able to do a poster in
% LaTeX. In
%
%    \begin{textblock}{wid}(x,y)
%    ...
%    \end{textblock}
%
% the first argument gives the block width in units of the grid
% cells specified above in \TPGrid; the second gives the (x,y)
% position on the grid, with the y axis pointing down.

% You will have to do a lot of previewing to get everything in the 
% right place.

% This gives good title positioning for a portrait poster.
% Watch out for hyphenation in titles - LaTeX will do it
% but it looks awful.
\begin{textblock}{23}(0,0)
\Title{NeuralODE studying}
\end{textblock}

\begin{textblock}{23}(0,0.6)
{
\LARGE
Ilya Lopatin
}

{
\Large
\color{bluegray}
\emph{Optimization Class Project. MIPT}
}
\end{textblock}


% Uni logo in the top right corner. A&A in the bottom left. Gives a
% good visual balance, but you may want to change this depending upon
% the graphics that are in your poster.
%\begin{textblock}{2}(0,10)
%Your logo here
%%\includegraphics{/usr/local/share/images/AandA.epsf}
%\end{textblock}

%\begin{textblock}{2}(21.2,0)
%Another logo here
%%\resizebox{2\TPHorizModule}{!}{\includegraphics{/usr/local/share/images/GUVIu/GUVIu.eps}}
%\end{textblock}


\begin{textblock}{7.0}(0,1.5)

\hrule\medskip
\Head{Introduction}\\
The NeuralODE architecture of was created in paper \cite{NeuralODE}, 2018. This project is devoted to studying of this neural network's architecture and carrying out numerical experiments with it.

\medskip
\hrule\medskip
\Head{The main idea of NeuralODE}\\
We can see that forward propagation in residual neural network looks like Euler's method of numerical solving of ordinary differential equation (1). It turns out that calculating gradient is presented as a solution of the other (\textit{adjoint}) ODE (2):
\begin{equation} \label{eq:forward_prop}
\left\{\begin{array}{lr}
         \frac{d z(t) }{dt}  = f(z(t), t, \theta), \\
         t   \in [t_1 ; t_2], \\
         z(t_1)  = x
        \end{array} \right. ,
\end{equation}
\begin{equation} \label{eq:adjoint}
\left\{ \begin{array}{lr}
  		 \frac{d a(t)}{dt} = - a^T(t) \frac{\partial f}{ z(t)} \\
		 a(t_2) = \frac{\partial L}{ \partial y}
\end{array} \right. , 
\end{equation}
where $ f(z(t), t, \theta)$ is representation of neural network, $\theta$ is set of trainable parameters and $L = L(y(x))$ is loss function. The euation (3) gives us possibility to calculate gradient $\nabla_{\theta} L $ and do the procedures of gradient descent.
\begin{equation}
 \nabla_{\theta} L = \int_{t_1}^{t_2} a^T(t) \frac{\partial f}{\partial \theta} \left( z(t), t, \theta \right) dt .
\end{equation}

\medskip
\hrule\medskip
\Head{Possible applications}
\begin{itemize}\itemsep=12pt
\item Replacing  Res-block with NeuralODE block. It can be fruitful because we replace Euler's scheme related to Res-block with more powerfull methods likes Runge-Kutta methodes or another methodes of  numerical solving ODE.   
\item  We get new method of restoring the hidden dynamics function by given set of observations. Dynamics function is right side of equation $z'(t) = f(z(t),t)$ and for NeuralODE restoring $f$ is standart training procedure.   
\item Let's have a look at task of modeling or sampling of unknown probability density function by given points. The most common methodes have $O(d^3)$  time asymptotics when $d$ is dimension of space. The following statement turns out to be useful (see \cite{NeuralODE} ): \\
\hspace{1.25 cm} \textit{Let $z(t)$ be a finite continuous random variable with probability $p(z(t))$ dependent on time. Let $ dz / dt = f(z(t), t)$ be a differential equation describing a continuous-in-time transformation of $z(t)$. Assuming that $f$ is uniformly Lipschitz continuous in $z$ and continuous in $t$, then the change in $\log$ probability also follows a differential equation (4) }:
\begin{equation}
 \frac{\partial \log p \left( z(t) \right) }{\partial t } = -tr \left( \frac{\partial f }{ \partial z(t) }   \right)
 \end{equation} 
Note that calculating matrix trace is $O(d)$. Thanks to (4) we can interpret  modeling or sampling of unknown probability density function as restoring the hidden dynamics function, see previous point.
\end{itemize}

\end{textblock}

\begin{textblock}{7.0}(8,1.5)
\hrule\medskip
\Head{Algorithm}\\
Below is an algorithm from \cite{NeuralODE} for calculating all the necessary gradients
\begin{figure}
\centering
\includegraphics[width = \linewidth]{figures/grad_alg.png}
\end{figure}
when $f_{aur}$ is augmented dynamic: $ \frac{ d \left[ z, \theta, t  \right] }{dt}  = f_{aug} \left( \left[ z, \theta, t \right]  \right) = \left[ f \left(  \left[ z, \theta, t \right] \right) , 0  , 1 \right] $

If we have some set of given observations or points of hidden distribution density, the adjoint sensitivity method solves an augmented ODE backwards in time. \begin{wrapfigure}{r}{0.7\linewidth}
\centering
\includegraphics[width = 0.7 \linewidth]{figures/backprop.png}
\caption{}
\end{wrapfigure} The augmented system contains both the original state and the sensitivity of the loss with respect to the state. If the loss depends directly on the state at multiple observation times, the adjoint state must be updated in the direction of the partial derivative of the loss with respect to each observation, see figure 1.

\medskip
\hrule\medskip
\Head{MNIST test}\\
The first experiments were devoted to classic MNIST test of recognizing numbers. In figure 2 we can see result of this expiriment. 

\begin{wrapfigure}{r}{0.7\linewidth}
\centering
\includegraphics[width = 0.8 \linewidth]{figures/MNIST_test.png}
\caption{}
\end{wrapfigure} $L$ denotes the number of layers in the ResNet, and $\tilde{L}$ is the number of function evaluations that the ODE solver requests in a single forward pass, which can be interpreted as an implicit number of layers.

Our experiments demonstrate that it takes 4 times longer for NeuralODE to achieve equivalent results with ResNet, but at the same time NeuralODE requires 3 times less trainable parameters and a constant quantity of memory. The runs were carried out under the same conditions in Google Coollaboratory with Google Compute Engine (GPU) support.

\end{textblock}

\begin{textblock}{7.0}(16,1.5)

\hrule\medskip
\Head{Restoring the hidden dynamics function}\\
We've carried out experiments in case when  hidden dynamics is constant matrix, that is , the equation of dynamics is $ z' (t) =  A z $

\begin{figure}[h]
\begin{center}
\begin{minipage}[h]{0.27\linewidth}
\includegraphics[width=1\linewidth]{figures/stable_focus1.png}
$$ A = \begin{bmatrix}
-1 & -1 \\
1 & -1 \\
\end{bmatrix}
$$ %% подпись к рисунку
\label{ris:experimoriginal} %% метка рисунка для ссылки на него
\end{minipage}
\hfill
\begin{minipage}[h]{0.27\linewidth}
\includegraphics[width=1\linewidth]{figures/stable_node1.png}
$$ A = \begin{bmatrix}
-1 & 0 \\
0 & -2 \\
\end{bmatrix}
$$
\end{minipage}
\hfill
\begin{minipage}[h]{0.27\linewidth}
\includegraphics[width=1\linewidth]{figures/cirle1.png}
$$ 
A = \begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix}
$$
\end{minipage} 
\end{center}
\end{figure} 

Below is the process of learning Neural CODE on a set of random points for each of the three presented systems.

\begin{center}
\begin{minipage}[h]{0.3\linewidth}
\begin{frame}{Stable Focus}
  \animategraphics[loop,controls=play,width=\linewidth]{6}{figures/stable_focus/00-}{1}{9}
\end{frame}
\end{minipage}
\hfill
\begin{minipage}[h]{0.3\linewidth}
\begin{frame}{Stable Node}
  \animategraphics[loop,controls=play,width=\linewidth]{6}{figures/stable_node/00-}{1}{9}
\end{frame}
\end{minipage}
\hfill
\begin{minipage}[h]{0.3\linewidth}
\begin{frame}{Circle}
  \animategraphics[loop,controls=play,width=\linewidth]{6}{figures/circle/00-}{1}{9}
\end{frame}
\end{minipage}
\end{center}

Note that during the experiments, the following problem was found. If the matrix $A$ has an eigenvalue  with a positive real part, which means that the system is unstable, then the program crashes.

Author's comment: \textit{This underflow error indicates that the system is too stiff for an explicit method to solve. The step size needs to be extremely small in order to satisfy the desired tolerance. Ricky Chen }

\medskip
\hrule\medskip
\Head{Conclusion}\\
During our first experiments, we partially tested some of the advantages and possible applications of Neural CODE indicated in the first two paragraphs of the section "Possible applications". We got positive results on the MNIST test and in the task of restoring hidden dynamics in the case of a stable system.

\medskip
\hrule\medskip
\Head{Acknowledgements}\\
This material is based on paper \cite{NeuralODE}. The source code of the neural network was taken from \cite{author_git}. I would like to express my special gratitude to Danya Merkulov  for him initiative to make this project, references and advices.
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{textblock}

\end{document}